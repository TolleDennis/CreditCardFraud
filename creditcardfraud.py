# -*- coding: utf-8 -*-
"""creditCardFraud.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1734MJOIOXCAmcOVc3yn-tuxovXdsJCCj
"""

import numpy as np
import pandas as pd
from sklearn.metrics import accuracy_score,classification_report,confusion_matrix
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

#loading of the data to panda data frame
credit_card_fraud=pd.read_csv("SyntheticFraudData.csv")
credit_card_fraud

#analysing the statistics of data
credit_card_fraud.describe()

#getting the info 
credit_card_fraud.info()

#checking for empty columns on the dataset
credit_card_fraud['isFraud'].isnull()

#the sum of all null columns in  the entire row
credit_card_fraud.isnull().sum()

#checking for the fraud and legit transactions value counts
credit_card_fraud['isFraud'].value_counts()

#dropping the row containing null values
credit_card_fraud.drop([3519656],inplace=True)

#confirming the dropped row
credit_card_fraud.isnull()

#separating data for analysis
legitTransaction = credit_card_fraud[credit_card_fraud['isFraud']==0]
fradTransaction = credit_card_fraud[credit_card_fraud['isFraud']==1]

#reading the transations
print(legitTransaction)
print(fradTransaction)

#statistical measures of data
legitTransaction['amount'].describe()

#statistical measures of data
legitTransaction['amount'].describe()

#checking the statistical distribution of data
credit_card_fraud.groupby('isFraud').mean()

#transaction counts for legit and fraud 
legitTransaction.count()

fradTransaction.count()

#Sample dataset containing similar distribution of legit transacton and the fraudent
#legit transactions are 3516747
#fradulent transactions 2909

legitTransactionSample=legitTransaction.sample(n=2909)

legitTransactionSample.count()

fradTransaction.count()

#Concat two dataframes
new_CreditCard=pd.concat([legitTransactionSample,fradTransaction],axis=0)

#reading the new dataframe
new_CreditCard

#confirming the change in statistical distribution
new_CreditCard.describe()

new_CreditCard.info()

#confirming statistical distribution by group by
new_CreditCard.groupby('isFraud').mean()

#Splitting the data into Features and targets,and droping the non numerical columns
X=new_CreditCard.drop(columns=['isFraud','type','nameOrig','nameDest'],axis=1)
Y=new_CreditCard['isFraud']

#printing the new data
X

#Dividing the data into training and test data
X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.25,stratify=Y,random_state=4)

#confirming the data
print(X_train)
print(Y_train)

print(Y_test)
print(X_test)

#Modelling Training with Logistic Regression Model
model = LogisticRegression(C=1e5)

#Training the logistic regression model with training data
print(X_train)

X_train.columns

#Training the logistic regression model with training data
model.fit(X_train,Y_train)

X

#Model Evaluation with Accuracy Score
#accuracy on training data
X_train_prediction = np.array(model.predict(X_train))
training_data_accuracy =np.array(accuracy_score(X_train_prediction,Y_train))

print('Our training data accuracy model score is: ',training_data_accuracy)

#accuracy on the test data
X_test_prediction = np.array(model.predict(X_test))
test_data_accuracy =np.array(accuracy_score(X_test_prediction,Y_test))

print('Our test data accuracy model score is: ',test_data_accuracy)

print(confusion_matrix(X_test_prediction,Y_test))

print(classification_report(X_test_prediction,Y_test))

